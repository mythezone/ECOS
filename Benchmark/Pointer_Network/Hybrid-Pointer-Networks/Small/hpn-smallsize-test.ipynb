{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-09-18T05:34:42.570231Z","iopub.status.busy":"2021-09-18T05:34:42.569774Z","iopub.status.idle":"2021-09-18T05:34:47.087975Z","shell.execute_reply":"2021-09-18T05:34:47.087258Z","shell.execute_reply.started":"2021-09-18T05:34:42.570113Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["import torch\n","import torch.nn as nn\n","import time\n","import argparse\n","\n","import os\n","import datetime\n","\n","from torch.distributions.categorical import Categorical\n","\n","# visualization \n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","device = torch.device(\"cpu\"); gpu_id = -1 # select CPU\n","\n","gpu_id = '0' # select a single GPU  \n","#gpu_id = '2,3' # select multiple GPUs  \n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)  \n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print('GPU name: {:s}, gpu_id: {:s}'.format(torch.cuda.get_device_name(0),gpu_id))   \n","    \n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["# Model's Components"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2021-09-18T05:34:47.089749Z","iopub.status.busy":"2021-09-18T05:34:47.089504Z","iopub.status.idle":"2021-09-18T05:34:47.141570Z","shell.execute_reply":"2021-09-18T05:34:47.140805Z","shell.execute_reply.started":"2021-09-18T05:34:47.089715Z"},"trusted":true},"outputs":[],"source":["import math\n","import numpy as np\n","import torch.nn.functional as F\n","import random\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.optim import lr_scheduler\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm_notebook\n","\n","class TransEncoderNet(nn.Module):\n","    \"\"\"\n","    Encoder network based on self-attention transformer\n","    Inputs :  \n","      h of size      (bsz, nb_nodes, dim_emb)    batch of input cities\n","    Outputs :  \n","      h of size      (bsz, nb_nodes, dim_emb)    batch of encoded cities\n","      score of size  (bsz, nb_nodes, nb_nodes+1) batch of attention scores\n","    \"\"\"\n","    \n","    def __init__(self, nb_layers, dim_emb, nb_heads, dim_ff, batchnorm):\n","        super(TransEncoderNet, self).__init__()\n","        assert dim_emb == nb_heads* (dim_emb//nb_heads) # check if dim_emb is divisible by nb_heads\n","        self.MHA_layers = nn.ModuleList( [nn.MultiheadAttention(dim_emb, nb_heads) for _ in range(nb_layers)] )\n","        self.linear1_layers = nn.ModuleList( [nn.Linear(dim_emb, dim_ff) for _ in range(nb_layers)] )\n","        self.linear2_layers = nn.ModuleList( [nn.Linear(dim_ff, dim_emb) for _ in range(nb_layers)] )   \n","        if batchnorm:\n","            self.norm1_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n","            self.norm2_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n","        else:\n","            self.norm1_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n","            self.norm2_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n","        self.nb_layers = nb_layers\n","        self.nb_heads = nb_heads\n","        self.batchnorm = batchnorm\n","        \n","    def forward(self, h):      \n","        # PyTorch nn.MultiheadAttention requires input size (seq_len, bsz, dim_emb) \n","        h = h.transpose(0,1) # size(h)=(nb_nodes, bsz, dim_emb)  \n","        # L layers\n","        for i in range(self.nb_layers):\n","            h_rc = h # residual connection, size(h_rc)=(nb_nodes, bsz, dim_emb)\n","            h, score = self.MHA_layers[i](h, h, h) # size(h)=(nb_nodes, bsz, dim_emb), size(score)=(bsz, nb_nodes, nb_nodes)\n","            # add residual connection\n","            \n","            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n","            if self.batchnorm:\n","                # Pytorch nn.BatchNorm1d requires input size (bsz, dim, seq_len)\n","                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n","                h = self.norm1_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n","                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n","            else:\n","                h = self.norm1_layers[i](h)       # size(h)=(nb_nodes, bsz, dim_emb) \n","            # feedforward\n","            h_rc = h # residual connection\n","            h = self.linear2_layers[i](torch.relu(self.linear1_layers[i](h)))\n","            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n","            if self.batchnorm:\n","                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n","                h = self.norm2_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n","                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n","            else:\n","                h = self.norm2_layers[i](h) # size(h)=(nb_nodes, bsz, dim_emb)\n","        # Transpose h\n","        h = h.transpose(0,1) # size(h)=(bsz, nb_nodes, dim_emb)\n","        return h, score\n","    \n","\n","class Attention(nn.Module):\n","    def __init__(self, n_hidden):\n","        super(Attention, self).__init__()\n","        self.size = 0\n","        self.batch_size = 0\n","        self.dim = n_hidden\n","        \n","        v  = torch.FloatTensor(n_hidden)\n","        self.v  = nn.Parameter(v)\n","        self.v.data.uniform_(-1/math.sqrt(n_hidden), 1/math.sqrt(n_hidden))\n","        \n","        # parameters for pointer attention\n","        self.Wref = nn.Linear(n_hidden, n_hidden)\n","        self.Wq = nn.Linear(n_hidden, n_hidden)\n","    \n","    \n","    def forward(self, q, ref):       # query and reference\n","        self.batch_size = q.size(0)\n","        self.size = int(ref.size(0) / self.batch_size)\n","        q = self.Wq(q)     # (B, dim)\n","        ref = self.Wref(ref)\n","        ref = ref.view(self.batch_size, self.size, self.dim)  # (B, size, dim)\n","        \n","        q_ex = q.unsqueeze(1).repeat(1, self.size, 1) # (B, size, dim)\n","        # v_view: (B, dim, 1)\n","        v_view = self.v.unsqueeze(0).expand(self.batch_size, self.dim).unsqueeze(2)\n","        \n","        # (B, size, dim) * (B, dim, 1)\n","        u = torch.bmm(torch.tanh(q_ex + ref), v_view).squeeze(2)\n","        \n","        return u, ref\n","    \n","class LSTM(nn.Module):\n","    def __init__(self, n_hidden):\n","        super(LSTM, self).__init__()\n","        \n","        # parameters for input gate\n","        self.Wxi = nn.Linear(n_hidden, n_hidden)    # W(xt)\n","        self.Whi = nn.Linear(n_hidden, n_hidden)    # W(ht)\n","        self.wci = nn.Linear(n_hidden, n_hidden)    # w(ct)\n","        \n","        # parameters for forget gate\n","        self.Wxf = nn.Linear(n_hidden, n_hidden)    # W(xt)\n","        self.Whf = nn.Linear(n_hidden, n_hidden)    # W(ht)\n","        self.wcf = nn.Linear(n_hidden, n_hidden)    # w(ct)\n","        \n","        # parameters for cell gate\n","        self.Wxc = nn.Linear(n_hidden, n_hidden)    # W(xt)\n","        self.Whc = nn.Linear(n_hidden, n_hidden)    # W(ht)\n","        \n","        # parameters for forget gate\n","        self.Wxo = nn.Linear(n_hidden, n_hidden)    # W(xt)\n","        self.Who = nn.Linear(n_hidden, n_hidden)    # W(ht)\n","        self.wco = nn.Linear(n_hidden, n_hidden)    # w(ct)\n","    \n","    \n","    def forward(self, x, h, c):       # query and reference\n","        \n","        # input gate\n","        i = torch.sigmoid(self.Wxi(x) + self.Whi(h) + self.wci(c))\n","        # forget gate\n","        f = torch.sigmoid(self.Wxf(x) + self.Whf(h) + self.wcf(c))\n","        # cell gate\n","        c = f * c + i * torch.tanh(self.Wxc(x) + self.Whc(h))\n","        # output gate\n","        o = torch.sigmoid(self.Wxo(x) + self.Who(h) + self.wco(c))\n","        \n","        h = o * torch.tanh(c)\n","        \n","        return h, c\n","\n","class HPN(nn.Module):\n","    def __init__(self, n_feature, n_hidden):\n","\n","        super(HPN, self).__init__()\n","        self.city_size = 0\n","        self.batch_size = 0\n","        self.dim = n_hidden\n","        \n","        # lstm for first turn\n","        #self.lstm0 = nn.LSTM(n_hidden, n_hidden)\n","        \n","        # pointer layer\n","        self.pointer = Attention(n_hidden)\n","        self.TransPointer = Attention(n_hidden)\n","        \n","        # lstm encoder\n","        self.encoder = LSTM(n_hidden)\n","        \n","        # trainable first hidden input\n","        h0 = torch.FloatTensor(n_hidden)\n","        c0 = torch.FloatTensor(n_hidden)\n","        \n","        # trainable latent variable coefficient\n","        alpha = torch.ones(1).cuda()\n","        \n","        self.h0 = nn.Parameter(h0)\n","        self.c0 = nn.Parameter(c0)\n","        \n","        self.alpha = nn.Parameter(alpha)\n","        self.h0.data.uniform_(-1/math.sqrt(n_hidden), 1/math.sqrt(n_hidden))\n","        self.c0.data.uniform_(-1/math.sqrt(n_hidden), 1/math.sqrt(n_hidden))\n","        \n","        r1 = torch.ones(1)\n","        r2 = torch.ones(1)\n","        r3 = torch.ones(1)\n","        self.r1 = nn.Parameter(r1)\n","        self.r2 = nn.Parameter(r2)\n","        self.r3 = nn.Parameter(r3)\n","        \n","        # embedding\n","        self.embedding_x = nn.Linear(n_feature, n_hidden)\n","        self.embedding_all = nn.Linear(n_feature, n_hidden)\n","        self.Transembedding_all = TransEncoderNet(6, 128, 8, 512, batchnorm=True)\n","        \n","        # vector to start decoding \n","        self.start_placeholder = nn.Parameter(torch.randn(n_hidden))\n","        \n","        # weights for GNN\n","        self.W1 = nn.Linear(n_hidden, n_hidden)\n","        self.W2 = nn.Linear(n_hidden, n_hidden)\n","        self.W3 = nn.Linear(n_hidden, n_hidden)\n","        \n","        # aggregation function for GNN\n","        self.agg_1 = nn.Linear(n_hidden, n_hidden)\n","        self.agg_2 = nn.Linear(n_hidden, n_hidden)\n","        self.agg_3 = nn.Linear(n_hidden, n_hidden)\n","    \n","    \n","    def forward(self,context,Transcontext, x, X_all, mask, h=None, c=None, latent=None):\n","        '''\n","        Inputs (B: batch size, size: city size, dim: hidden dimension)\n","        \n","        x: current city coordinate (B, 2)\n","        X_all: all cities' cooridnates (B, size, 2)\n","        mask: mask visited cities\n","        h: hidden variable (B, dim)\n","        c: cell gate (B, dim)\n","        latent: latent pointer vector from previous layer (B, size, dim)\n","        \n","        Outputs\n","        \n","        softmax: probability distribution of next city (B, size)\n","        h: hidden variable (B, dim)\n","        c: cell gate (B, dim)\n","        latent_u: latent pointer vector for next layer\n","        '''\n","        \n","        self.batch_size = X_all.size(0)\n","        self.city_size = X_all.size(1)\n","        \n","        # Check if this the first iteration loop\n","        if h is None or c is None:\n","            x          = self.start_placeholder    \n","            context = self.embedding_all(X_all)\n","            Transcontext,_ = self.Transembedding_all(context)\n","            \n","            # =============================\n","            # graph neural network encoder\n","            # =============================\n","\n","            # (B, size, dim)\n","            context = context.reshape(-1, self.dim)\n","            Transcontext = Transcontext.reshape(-1, self.dim)\n","\n","            context = self.r1 * self.W1(context)\\\n","                + (1-self.r1) * F.relu(self.agg_1(context/(self.city_size-1)))\n","\n","            context = self.r2 * self.W2(context)\\\n","                + (1-self.r2) * F.relu(self.agg_2(context/(self.city_size-1)))\n","\n","            context = self.r3 * self.W3(context)\\\n","                + (1-self.r3) * F.relu(self.agg_3(context/(self.city_size-1)))\n","            h0 = self.h0.unsqueeze(0).expand(self.batch_size, self.dim)\n","            c0 = self.c0.unsqueeze(0).expand(self.batch_size, self.dim)\n","\n","            h0 = h0.unsqueeze(0).contiguous()\n","            c0 = c0.unsqueeze(0).contiguous()\n","            \n","            # let h0, c0 be the hidden variable of first turn\n","            h = h0.squeeze(0)\n","            c = c0.squeeze(0)\n","        else:\n","            x          = self.embedding_x(x)\n","        # LSTM encoder\n","        h, c = self.encoder(x, h, c)\n","        # query vector\n","        q = h\n","        # pointer\n","        u1, _ = self.pointer(q, context)\n","        u2 ,_ = self.TransPointer(q,Transcontext)\n","        \n","        # Summing the two attention vectors\n","        u = u1 + u2\n","        latent_u = u.clone()\n","        u = 10 * torch.tanh(u) + mask\n","        return context,Transcontext,F.softmax(u, dim=1), h, c, latent_u"]},{"cell_type":"markdown","metadata":{},"source":["# TEST"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-09-18T05:36:47.764122Z","iopub.status.busy":"2021-09-18T05:36:47.763808Z","iopub.status.idle":"2021-09-18T05:36:49.771983Z","shell.execute_reply":"2021-09-18T05:36:49.771260Z","shell.execute_reply.started":"2021-09-18T05:36:47.764092Z"},"trusted":true},"outputs":[{"ename":"AssertionError","evalue":"Torch not compiled with CUDA enabled","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-a88bce2f6e6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Instantiate a training network and a baseline network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m###################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mCritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHPN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mCritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-44ab1a09647c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_feature, n_hidden)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# trainable latent variable coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/envs/GraphDTLP/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise AssertionError(\n","\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"]}],"source":["###################\n","# Instantiate a training network and a baseline network\n","###################\n","Critic = HPN(n_feature=2, n_hidden=128).to(device)\n","Critic.eval()\n","\n","#********************************************# Uncomment these lines to re-start training with saved checkpoint #********************************************#\n","checkpoint_file = \"../input/tsp50beamsearch/checkpoint_21-09-03--20-13-13-n50-gpu0.pkl\"\n","checkpoint = torch.load(checkpoint_file, map_location=device)\n","Critic.load_state_dict(checkpoint['model_baseline'])\n","del checkpoint\n","#*********************************************# Uncomment these lines to re-start training with saved checkpoint #********************************************#\n","Checkpoint = True\n","#Checkpoint = False\n","\n","B_val = 10000\n","size_val = 50\n","\n","if Checkpoint:\n","    X = torch.load('../input/tsp50beamsearch/10k_TSP50.pt').to(device)\n","else:\n","    X = torch.rand(B_val,size_val,2, device = device)\n","    \n","with torch.no_grad():\n","    # greedy validation\n","    tour_len = 0\n","    zero_to_bsz = torch.arange(B_val, device = device) # [0,1,...,bsz-1]\n","    mask = torch.zeros(B_val,size_val).cuda()\n","    R = 0\n","    reward = 0\n","    Y = X.view(B_val, size_val, 2)    # to the same batch size\n","    x = Y[:,0,:]\n","    h = None\n","    c = None\n","    context = None\n","    Transcontext = None\n","    for k in range(size_val):\n","        context,Transcontext,output, h, c, _ = Critic(context,Transcontext,x=x, X_all=X, h=h, c=c, mask=mask)\n","        idx = torch.argmax(output, dim=1)\n","        Y1 = Y[zero_to_bsz, idx.data]\n","        if k == 0:\n","            Y_ini = Y1.clone()\n","        if k > 0:\n","            #reward = torch.linalg.norm(Y1-Y0, dim=1)\n","            reward  = torch.sum((Y1 - Y0)**2 , dim=1 )**0.5\n","        Y0 = Y1.clone()\n","        x = Y[zero_to_bsz, idx.data]\n","        R += reward\n","        mask[zero_to_bsz, idx.data] += -np.inf\n","\n","    #R += torch.linalg.norm(Y1-Y_ini, dim=1)\n","    R  += torch.sum((Y1 - Y_ini)**2 , dim=1 )**0.5\n","    tour_len += R.mean().item()\n","    print('validation tour length:', tour_len)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["'/Users/zhongmuyao/Library/Mobile Documents/com~apple~CloudDocs/Project.nosync/Github/ECOS/Benchmark/Pointer_Network/Hybrid-Pointer-Networks/Small'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["pwd"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"nbformat":4,"nbformat_minor":4}
