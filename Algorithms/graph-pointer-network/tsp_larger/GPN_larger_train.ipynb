{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bljcWn6nFXMg"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DhLCwUd9GJv3"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFcsK8CpFXJ0"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(Attention, self).__init__()\n",
    "        self.size = 0\n",
    "        self.batch_size = 0\n",
    "        self.dim = n_hidden\n",
    "        \n",
    "        v  = torch.FloatTensor(n_hidden).cuda()\n",
    "        self.v  = nn.Parameter(v)\n",
    "        self.v.data.uniform_(-1/math.sqrt(n_hidden), 1/math.sqrt(n_hidden))\n",
    "        \n",
    "        # parameters for pointer attention\n",
    "        self.Wref = nn.Linear(n_hidden, n_hidden)\n",
    "        self.Wq = nn.Linear(n_hidden, n_hidden)\n",
    "    \n",
    "    \n",
    "    def forward(self, q, ref):       # query and reference\n",
    "        self.batch_size = q.size(0)\n",
    "        self.size = int(ref.size(0) / self.batch_size)\n",
    "        q = self.Wq(q)     # (B, dim)\n",
    "        ref = self.Wref(ref)\n",
    "        ref = ref.view(self.batch_size, self.size, self.dim)  # (B, size, dim)\n",
    "        \n",
    "        q_ex = q.unsqueeze(1).repeat(1, self.size, 1) # (B, size, dim)\n",
    "        # v_view: (B, dim, 1)\n",
    "        v_view = self.v.unsqueeze(0).expand(self.batch_size, self.dim).unsqueeze(2)\n",
    "        \n",
    "        # (B, size, dim) * (B, dim, 1)\n",
    "        u = torch.bmm(torch.tanh(q_ex + ref), v_view).squeeze(2)\n",
    "        \n",
    "        return u, ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QmEvDZMCFXGl"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        # parameters for input gate\n",
    "        self.Wxi = nn.Linear(n_hidden, n_hidden)    # W(xt)\n",
    "        self.Whi = nn.Linear(n_hidden, n_hidden)    # W(ht)\n",
    "        self.wci = nn.Linear(n_hidden, n_hidden)    # w(ct)\n",
    "        \n",
    "        # parameters for forget gate\n",
    "        self.Wxf = nn.Linear(n_hidden, n_hidden)    # W(xt)\n",
    "        self.Whf = nn.Linear(n_hidden, n_hidden)    # W(ht)\n",
    "        self.wcf = nn.Linear(n_hidden, n_hidden)    # w(ct)\n",
    "        \n",
    "        # parameters for cell gate\n",
    "        self.Wxc = nn.Linear(n_hidden, n_hidden)    # W(xt)\n",
    "        self.Whc = nn.Linear(n_hidden, n_hidden)    # W(ht)\n",
    "        \n",
    "        # parameters for forget gate\n",
    "        self.Wxo = nn.Linear(n_hidden, n_hidden)    # W(xt)\n",
    "        self.Who = nn.Linear(n_hidden, n_hidden)    # W(ht)\n",
    "        self.wco = nn.Linear(n_hidden, n_hidden)    # w(ct)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, h, c):       # query and reference\n",
    "        \n",
    "        # input gate\n",
    "        i = torch.sigmoid(self.Wxi(x) + self.Whi(h) + self.wci(c))\n",
    "        # forget gate\n",
    "        f = torch.sigmoid(self.Wxf(x) + self.Whf(h) + self.wcf(c))\n",
    "        # cell gate\n",
    "        c = f * c + i * torch.tanh(self.Wxc(x) + self.Whc(h))\n",
    "        # output gate\n",
    "        o = torch.sigmoid(self.Wxo(x) + self.Who(h) + self.wco(c))\n",
    "        \n",
    "        h = o * torch.tanh(c)\n",
    "        \n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ukWUrRldFXDd"
   },
   "outputs": [],
   "source": [
    "class GPN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_feature, n_hidden):\n",
    "        super(GPN, self).__init__()\n",
    "        self.city_size = 0\n",
    "        self.batch_size = 0\n",
    "        self.dim = n_hidden\n",
    "        \n",
    "        # lstm for first turn\n",
    "        self.lstm0 = nn.LSTM(n_hidden, n_hidden)\n",
    "        \n",
    "        # pointer layer\n",
    "        self.pointer = Attention(n_hidden)\n",
    "        \n",
    "        # lstm encoder\n",
    "        self.encoder = LSTM(n_hidden)\n",
    "        \n",
    "        # trainable first hidden input\n",
    "        h0 = torch.FloatTensor(n_hidden).cuda()\n",
    "        c0 = torch.FloatTensor(n_hidden).cuda()\n",
    "        \n",
    "        # trainable latent variable coefficient\n",
    "        alpha = torch.ones(1).cuda()\n",
    "        \n",
    "        self.h0 = nn.Parameter(h0)\n",
    "        self.c0 = nn.Parameter(c0)\n",
    "        \n",
    "        self.alpha = nn.Parameter(alpha)\n",
    "        self.h0.data.uniform_(-1/math.sqrt(n_hidden), 1/math.sqrt(n_hidden))\n",
    "        self.c0.data.uniform_(-1/math.sqrt(n_hidden), 1/math.sqrt(n_hidden))\n",
    "        \n",
    "        r1 = torch.ones(1).cuda()\n",
    "        r2 = torch.ones(1).cuda()\n",
    "        r3 = torch.ones(1).cuda()\n",
    "        self.r1 = nn.Parameter(r1)\n",
    "        self.r2 = nn.Parameter(r2)\n",
    "        self.r3 = nn.Parameter(r3)\n",
    "        \n",
    "        # embedding\n",
    "        self.embedding_x = nn.Linear(n_feature, n_hidden)\n",
    "        self.embedding_all = nn.Linear(n_feature, n_hidden)\n",
    "        \n",
    "        \n",
    "        # weights for GNN\n",
    "        self.W1 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.W2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.W3 = nn.Linear(n_hidden, n_hidden)\n",
    "        \n",
    "        # aggregation function for GNN\n",
    "        self.agg_1 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.agg_2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.agg_3 = nn.Linear(n_hidden, n_hidden)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, X_all, mask, h=None, c=None, latent=None):\n",
    "        '''\n",
    "        Inputs (B: batch size, size: city size, dim: hidden dimension)\n",
    "        \n",
    "        x: current city coordinate (B, 2)\n",
    "        X_all: all cities' cooridnates (B, size, 2)\n",
    "        mask: mask visited cities\n",
    "        h: hidden variable (B, dim)\n",
    "        c: cell gate (B, dim)\n",
    "        latent: latent pointer vector from previous layer (B, size, dim)\n",
    "        \n",
    "        Outputs\n",
    "        \n",
    "        softmax: probability distribution of next city (B, size)\n",
    "        h: hidden variable (B, dim)\n",
    "        c: cell gate (B, dim)\n",
    "        latent_u: latent pointer vector for next layer\n",
    "        '''\n",
    "        \n",
    "        self.batch_size = X_all.size(0)\n",
    "        self.city_size = X_all.size(1)\n",
    "        \n",
    "        \n",
    "        # =============================\n",
    "        # vector context\n",
    "        # =============================\n",
    "        \n",
    "        x_expand = x.unsqueeze(1).repeat(1, self.city_size, 1)   # (B, size)\n",
    "        X_all = X_all - x_expand\n",
    "        \n",
    "        # the weights share across all the cities\n",
    "        x = self.embedding_x(x)\n",
    "        context = self.embedding_all(X_all)\n",
    "        \n",
    "        # =============================\n",
    "        # process hidden variable\n",
    "        # =============================\n",
    "        \n",
    "        first_turn = False\n",
    "        if h is None or c is None:\n",
    "            first_turn = True\n",
    "        \n",
    "        if first_turn:\n",
    "            # (dim) -> (B, dim)\n",
    "            \n",
    "            h0 = self.h0.unsqueeze(0).expand(self.batch_size, self.dim)\n",
    "            c0 = self.c0.unsqueeze(0).expand(self.batch_size, self.dim)\n",
    "\n",
    "            h0 = h0.unsqueeze(0).contiguous()\n",
    "            c0 = c0.unsqueeze(0).contiguous()\n",
    "            \n",
    "            input_context = context.permute(1,0,2).contiguous()\n",
    "            _, (h_enc, c_enc) = self.lstm0(input_context, (h0, c0))\n",
    "            \n",
    "            # let h0, c0 be the hidden variable of first turn\n",
    "            h = h_enc.squeeze(0)\n",
    "            c = c_enc.squeeze(0)\n",
    "        \n",
    "        \n",
    "        # =============================\n",
    "        # graph neural network encoder\n",
    "        # =============================\n",
    "        \n",
    "        # (B, size, dim)\n",
    "        context = context.view(-1, self.dim)\n",
    "        \n",
    "        context = self.r1 * self.W1(context)\\\n",
    "            + (1-self.r1) * F.relu(self.agg_1(context/(self.city_size-1)))\n",
    "\n",
    "        context = self.r2 * self.W2(context)\\\n",
    "            + (1-self.r2) * F.relu(self.agg_2(context/(self.city_size-1)))\n",
    "        \n",
    "        context = self.r3 * self.W3(context)\\\n",
    "            + (1-self.r3) * F.relu(self.agg_3(context/(self.city_size-1)))\n",
    "        \n",
    "        \n",
    "        # LSTM encoder\n",
    "        h, c = self.encoder(x, h, c)\n",
    "        \n",
    "        # query vector\n",
    "        q = h\n",
    "        \n",
    "        # pointer\n",
    "        u, _ = self.pointer(q, context)\n",
    "        \n",
    "        latent_u = u.clone()\n",
    "        \n",
    "        u = 100 * torch.tanh(u) + mask\n",
    "        \n",
    "        if latent is not None:\n",
    "            u += self.alpha * latent\n",
    "    \n",
    "        return F.softmax(u, dim=1), h, c, latent_u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uElwDzaOGkAT"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "9BT073y3FW9P",
    "outputId": "357d52e4-7654-48a4-a4df-d438a4090b7c"
   },
   "outputs": [],
   "source": [
    "size = 50\n",
    "learn_rate = 1e-3    # learning rate\n",
    "B = 128    # batch_size\n",
    "B_val = 32     # validation size\n",
    "size_val = 500\n",
    "steps = 2500    # training steps\n",
    "n_epoch = 10    # epochs\n",
    "save_root = './model/gpn_tsp500.pt'\n",
    "\n",
    "print('=========================')\n",
    "print('prepare to train')\n",
    "print('=========================')\n",
    "print('Hyperparameters:')\n",
    "print('size', size)\n",
    "print('learning rate', learn_rate)\n",
    "print('batch size', B)\n",
    "print('validation size', B_val)\n",
    "print('steps', steps)\n",
    "print('epoch', n_epoch)\n",
    "print('save root:', save_root)\n",
    "print('=========================')\n",
    "\n",
    "model = GPN(n_feature=2, n_hidden=128).cuda()\n",
    "\n",
    "# load model\n",
    "# model = torch.load(save_root).cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "lr_decay_step = 2500\n",
    "lr_decay_rate = 0.96\n",
    "opt_scheduler = lr_scheduler.MultiStepLR(optimizer, range(lr_decay_step, lr_decay_step*1000,\n",
    "                                     lr_decay_step), gamma=lr_decay_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qyDDTG6uFW6O",
    "outputId": "628c7802-abac-439c-e3a8-462fbc9be985"
   },
   "outputs": [],
   "source": [
    "C = 0     # baseline\n",
    "R = 0     # reward\n",
    "\n",
    "val_mean = []\n",
    "val_std = []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    for i in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        X = np.random.rand(B, size, 2)        \n",
    "    \n",
    "        X = torch.Tensor(X).cuda()\n",
    "        \n",
    "        mask = torch.zeros(B,size).cuda()\n",
    "    \n",
    "        R = 0\n",
    "        logprobs = 0\n",
    "        reward = 0\n",
    "        \n",
    "        Y = X.view(B,size,2)\n",
    "        x = Y[:,0,:]\n",
    "        h = None\n",
    "        c = None\n",
    "    \n",
    "        for k in range(size):\n",
    "            \n",
    "            output, h, c, _ = model(x=x, X_all=X, h=h, c=c, mask=mask)\n",
    "            \n",
    "            sampler = torch.distributions.Categorical(output)\n",
    "            idx = sampler.sample()         # now the idx has B elements\n",
    "    \n",
    "            Y1 = Y[[i for i in range(B)], idx.data].clone()\n",
    "            if k == 0:\n",
    "                Y_ini = Y1.clone()\n",
    "            if k > 0:\n",
    "                reward = torch.norm(Y1-Y0, dim=1)\n",
    "                \n",
    "            Y0 = Y1.clone()\n",
    "            x = Y[[i for i in range(B)], idx.data].clone()\n",
    "            \n",
    "            R += reward\n",
    "                \n",
    "            TINY = 1e-15\n",
    "            logprobs += torch.log(output[[i for i in range(B)], idx.data]+TINY) \n",
    "            \n",
    "            mask[[i for i in range(B)], idx.data] += -np.inf \n",
    "            \n",
    "        R += torch.norm(Y1-Y_ini, dim=1)\n",
    "        \n",
    "        \n",
    "        # self-critic base line\n",
    "        mask = torch.zeros(B,size).cuda()\n",
    "        \n",
    "        C = 0\n",
    "        baseline = 0\n",
    "        \n",
    "        Y = X.view(B,size,2)\n",
    "        x = Y[:,0,:]\n",
    "        h = None\n",
    "        c = None\n",
    "        \n",
    "        for k in range(size):\n",
    "        \n",
    "            output, h, c, _ = model(x=x, X_all=X, h=h, c=c, mask=mask)\n",
    "        \n",
    "            # sampler = torch.distributions.Categorical(output)\n",
    "            # idx = sampler.sample()         # now the idx has B elements\n",
    "            idx = torch.argmax(output, dim=1)    # greedy baseline\n",
    "        \n",
    "            Y1 = Y[[i for i in range(B)], idx.data].clone()\n",
    "            if k == 0:\n",
    "                Y_ini = Y1.clone()\n",
    "            if k > 0:\n",
    "                baseline = torch.norm(Y1-Y0, dim=1)\n",
    "        \n",
    "            Y0 = Y1.clone()\n",
    "            x = Y[[i for i in range(B)], idx.data].clone()\n",
    "        \n",
    "            C += baseline\n",
    "            mask[[i for i in range(B)], idx.data] += -np.inf\n",
    "    \n",
    "        C += torch.norm(Y1-Y_ini, dim=1)\n",
    "    \n",
    "        gap = (R-C).mean()\n",
    "        loss = ((R-C-gap)*logprobs).mean()\n",
    "    \n",
    "        loss.backward()\n",
    "        \n",
    "        max_grad_norm = 1.0\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                           max_grad_norm, norm_type=2)\n",
    "        optimizer.step()\n",
    "        opt_scheduler.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(\"epoch:{}, batch:{}/{}, reward:{}\"\n",
    "                .format(epoch, i, steps, R.mean().item()))\n",
    "            # R_mean.append(R.mean().item())\n",
    "            # R_std.append(R.std().item())\n",
    "\n",
    "            # greedy validation\n",
    "            \n",
    "            tour_len = 0\n",
    "\n",
    "            X_val = np.random.rand(B_val, size_val, 2)\n",
    "            X = X_val\n",
    "            X = torch.Tensor(X).cuda()\n",
    "            \n",
    "            mask = torch.zeros(B_val,size_val).cuda()\n",
    "            \n",
    "            R = 0\n",
    "            logprobs = 0\n",
    "            Idx = []\n",
    "            reward = 0\n",
    "            \n",
    "            Y = X.view(B_val, size_val, 2)    # to the same batch size\n",
    "            x = Y[:,0,:]\n",
    "            h = None\n",
    "            c = None\n",
    "            \n",
    "            for k in range(size_val):\n",
    "                \n",
    "                output, h, c, hidden_u = model(x=x, X_all=X, h=h, c=c, mask=mask)\n",
    "                \n",
    "                sampler = torch.distributions.Categorical(output)\n",
    "                # idx = sampler.sample()\n",
    "                idx = torch.argmax(output, dim=1)\n",
    "                Idx.append(idx.data)\n",
    "            \n",
    "                Y1 = Y[[i for i in range(B_val)], idx.data]\n",
    "                \n",
    "                if k == 0:\n",
    "                    Y_ini = Y1.clone()\n",
    "                if k > 0:\n",
    "                    reward = torch.norm(Y1-Y0, dim=1)\n",
    "        \n",
    "                Y0 = Y1.clone()\n",
    "                x = Y[[i for i in range(B_val)], idx.data]\n",
    "                \n",
    "                R += reward\n",
    "                \n",
    "                mask[[i for i in range(B_val)], idx.data] += -np.inf\n",
    "        \n",
    "            R += torch.norm(Y1-Y_ini, dim=1)\n",
    "            \n",
    "            val_mean.append(R.mean().item())\n",
    "            val_std.append(R.std().item())\n",
    "            tour_len += R.mean().item()\n",
    "            print('validation tour length:', tour_len)\n",
    "\n",
    "    print('save model to: ', save_root)\n",
    "    torch.save(model, save_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0qRCpl3YC_ve"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GPN-500-TSP50-find-best.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
